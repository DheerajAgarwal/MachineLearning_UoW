{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Ridge Regression & Quiz Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "\n",
    "Which of the following is NOT a valid measure of overfitting?\n",
    "\n",
    "* Sum of parameters $(w_1+w_2+...+w_n)$\n",
    "* Sum of squares of parameters $(w_1^2+w_2^2+…+w^2_n)$\n",
    "* Range of parameters, i.e., difference between maximum and minimum parameters\n",
    "* Sum of absolute values of parameters $(|w_1|+|w_2|+…+|w_n|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** Sum of parameters $(w_1+w_2+...+w_n)$\n",
    "    \n",
    "This is not a valid measure because it creates a _netting_ effect where two errors of opposite sign tend to cancel each other out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "\n",
    "In ridge regression, choosing a large penalty strength λ tends to lead to a model with (choose all that apply):  \n",
    "\n",
    "* High Bias\n",
    "* Low Bias\n",
    "* High Variance\n",
    "* Low Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** High Bis and Low Variance\n",
    "\n",
    "Increaing lambda all the way up to infinity, in that limit, we get coefficients shrunk to be zero. That is the model tends to be a very simplified one and leads to high bias. However this simplicity is a trade off that leads to low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "\n",
    "Which of the following plots best characterize the trend of bias, variance, and generalization error (all plotted over $\\lambda$)?\n",
    "\n",
    "* A ![A](img/Week4_lambda_impact_option_a.png)\n",
    "* B ![B](img/Week4_lambda_impact_option_b.png)\n",
    "* C ![C](img/Week4_lambda_impact_option_c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** C\n",
    "\n",
    "Refer answer to question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "\n",
    "In ridge regression using unnormalized features, if you double the value of a given feature (i.e., a specific column of the feature matrix), what happens to the estimated coefficients for every other feature? They:\n",
    "* Double\n",
    "* Half\n",
    "* Stay the same\n",
    "* impossible to tell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** Either Double or Impossible to tell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "\n",
    "If we only have a small number of observations, K-fold cross validation provides a better estimate of the generalization error than the validation set method.\n",
    "* TRUE\n",
    "* FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6:\n",
    "\n",
    "10-fold cross validation is more computationally intensive than leave-one-out (LOO) cross validation.\n",
    "* TRUE\n",
    "* FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7:\n",
    "Assume you have a training dataset consisting of $N$ observations and $D$ features. You use the closed-form solution to fit a multiple linear regression model using ridge regression. To choose the penalty strength $\\lambda$, you run leave-one-out (LOO) cross validation searching over $L$ values of $\\lambda$. Let $Cost(N,D)$ be the computational cost of running ridge regression with $N$ data points and $D$ features. Assume the prediction cost is negligible compared to the computational cost of training the model. \n",
    "\n",
    "Which of the following represents the computational cost of your LOO cross validation procedure?\n",
    "* $LN⋅Cost(N,D)$\n",
    "* $LD⋅Cost(N-1,D)$\n",
    "* $LD⋅Cost(N,D)$\n",
    "* $L⋅Cost(N-1,D)$\n",
    "* $L⋅Cost(N,D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** $LD⋅Cost(N-1,D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8:\n",
    "\n",
    "Assume you have a training dataset consisting of 1 million observations. Suppose running the closed-form solution to fit a multiple linear regression model using ridge regression on this data takes 1 second. Suppose you want to choose the penalty strength $\\lambda$ by searching over 100 possible values. \n",
    "\n",
    "How long will it take to run leave-one-out (LOO) cross-validation for this selection task?\n",
    "* ~3 hours\n",
    "* ~3 days\n",
    "* ~3 years\n",
    "* ~3 decades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** ~3 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9:\n",
    "\n",
    "Assume you have a training dataset consisting of 1 million observations. Suppose running the closed-form solution to fit a multiple linear regression model using ridge regression on this data takes 1 second. Suppose you want to choose the penalty strength $\\lambda$ by searching over 100 possible values. \n",
    "\n",
    "If you only want to spend about 1 hour to select $\\lambda$, what value of $k$ should you use for $k-fold$ cross-validation?\n",
    "* $k = 6$\n",
    "* $k = 36$\n",
    "* $k = 600$\n",
    "* $k = 3600$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** $k = 36$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gl-env]",
   "language": "python",
   "name": "conda-env-gl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
