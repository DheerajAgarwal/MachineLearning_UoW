{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Week 3: Understanding Error Types & Assessing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will re-enforce our undertsanding of types of error and how they relate to model complexity. This is only at a conceptual level and hence no code is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "If the features of Model 1 are a strict subset of those in Model 2, the TRAINING error of the two models can never be the same.\n",
    "* TRUE\n",
    "* FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** FALSE.\n",
    "\n",
    "If the features of Model 1 are strictly the subset of those in Model 2, the _TRAINING_ error os the two model can be same depending on what the error introduced because of the delta features. The delta features may or may not contibute to additional error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "If the features of Model 1 are a strict subset of those in Model 2, which model will USUALLY have lowest TRAINING error?\n",
    "* Model 1\n",
    "* Model 2\n",
    "* Impossible to tell from this information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** Model 2\n",
    "\n",
    "The number of features _typically_ increases the model complexity. Adding more features also tend to explain the behavior a little more than if lesser number of features are used. Given this, on training data set, the model with greater number of features will perform usually better than the model with lesser features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "If the features of Model 1 are a strict subset of those in Model 2. which model will USUALLY have lowest TEST error?\n",
    "* Model 1\n",
    "* Model 2\n",
    "* Impossible to tell from this information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** Impossible to tell from this information alone.\n",
    "\n",
    "From the answer of question 2, it may be a little counter intuitive to see why it is not possible to tell which model will perform better. The model complexity, tends to explain the data provided better, which is the _training_ data set. When it comes to _test_ data set the model may or may not perform well because, the model may or may not generalize well. With greater complexity comes greater variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "If the features of Model 1 are a strict subset of those in Model 2, which model will USUALLY have lower BIAS?\n",
    "* Model 1\n",
    "* Model 2\n",
    "* Impossible to tell from this information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** Model 2  \n",
    "\n",
    "_BIAS_ is the difference unexplained **expected **difference between the square of _true_ fit and the _model_ fit. \n",
    "\n",
    "$$E[f-\\hat{f}]^2$$\n",
    "where $f$ is the _true_ fit and $\\hat{f}$ is the _model_ fit.\n",
    "\n",
    "Since Model 2 has more features (more complexity) and leads to lesser _training_ error, Model 2 will _ty[ically_ also have lower bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Which of the following plots of model complexity vs. RSS is most likely from TRAINING data (for a fixed data set)?\n",
    "\n",
    "![title](img/model_complexity_vs_rss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** C\n",
    "\n",
    "Based on what we have discussed above for question 2 and then for question 4, the answer is self-explanatory here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Which of the following plots of model complexity vs. RSS is most likely from TEST data (for a fixed data set)?\n",
    "\n",
    "![title](img/model_complexity_vs_rss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** A\n",
    "\n",
    "Based on what we have discussed above for question 2, 3 and then for question 4, the answer is self-explanatory here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Is it always optimal to add more features to a regression model?\n",
    "* TRUE\n",
    "* FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** FALSE\n",
    "\n",
    "Adding more features _usually_ would lead to lower error however the model complexity and its error behaves differently for _training_ and the _test_ sets. (Refer question 5 & 6 above). Given that behavior, the optimal performace is when **both** the training and the test errors are minimum. Any further complexity beyond that combined local minima does not help with model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "A simple model with few parameters is most likely to suffer from:\n",
    "* High Bias\n",
    "* High Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** High Bias\n",
    "    \n",
    "Refer to explanation for question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "A complex model with many parameters is most likely to suffer from:\n",
    "* High Bias\n",
    "* High Variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** High Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "A model with many parameters that fits training data very well but does poorly on test data is considered to be\n",
    "* accurate\n",
    "* biased\n",
    "* overfitted\n",
    "* poorly estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** overfitted\n",
    "    \n",
    "Refer to graph a and c for question 5 above. If a model is towards the right of the mimima for test error and also to the right of the training error, then it is over fitted. It is an underfit if the training and the test errors are to the left of the minimum on test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "A common process for selecting a parameter like the optimal polynomial degree is:\n",
    "* Bootstrapping\n",
    "* Model estimation\n",
    "* Multiple regression\n",
    "* Minimizing test error\n",
    "* Minimizing validation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** Minimizing Validation error\n",
    "\n",
    "If the validation error is minimzed it provides a good approximation for generalization error and hence should be used for parameter selection even though the _training_ or the _test_ error may not be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "Selecting model complexity on test data (choose all that apply):\n",
    "\n",
    "* Allows you to avoid issues of overfitting to training data\n",
    "* Provides an overly optimistic assessment of performance of the resulting model\n",
    "* Is computationally inefficient\n",
    "* Should never be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS:** Provides an overly optimistic assessment and hence should never be done.\n",
    "    \n",
    "Refer to answer for question 11. Also, IMHO, this is a poorly phrased question for expecting multiple answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "Which of the following statements is true (select all that apply): For a fixed model complexity, in the limit of an infinite amount of training data:\n",
    "* The noise goes to 0\n",
    "* Bias goes to 0\n",
    "* Variance goes to 0\n",
    "* Training error goes to 0\n",
    "* Generalization error goes to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANS** Variance goes to 0\n",
    "\n",
    "Variance is a guard against extreme complexity of the model:\n",
    "$$E[y^2]$$\n",
    "$$ = var[\\hat{f}] $$\n",
    "\n",
    "If the _training_ set is the complete universe of such data then $\\hat{f} -> f$ and hence will have zero variance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gl-env]",
   "language": "python",
   "name": "conda-env-gl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
